---
title: "Individual"
author: "Yuanyuan Qu"
date: "11/9/2019"
output: html_document
---
##### (a)
```{r}
library(ISLR)
fix(OJ)
attach(OJ)
set.seed(1)
train=sample(nrow(OJ),800,replace = F)
train.oj=OJ[train,]
test.oj=OJ[-train,]
```
##### (b)
```{r}
library(e1071)
attach(OJ)
svcfit=svm(Purchase~.,data=train.oj,kernel="linear", cost=0.01)
summary(svcfit)
```
##### According to the result, there are 435 supporting vectors in total. Among them, There are two classes, one is CH, the other is MM. There are 219 supporting vectors on one side and 216 on the other side.

##### (c)
```{r}
ypred.train=predict(svcfit,train.oj)
mean(ypred.train!=OJ$Purchase[train])
ypred.test=predict(svcfit,test.oj)
mean(ypred.test!=test.oj$Purchase) 
```
##### The training error is 0.175, the test error is 0.178.

##### (d)
```{r}
tune.out = tune(svm, Purchase~., data = OJ[train,], kernel = "linear", ranges=list(cost=c(0.01,0.1,1,10)))
bestmod=tune.out$best.model
summary(bestmod)
```
##### (e)
```{r}
ypred.train2=predict(bestmod,train.oj)
mean(ypred.train2!=OJ$Purchase[train])
ypred.test2=predict(bestmod,test.oj)
mean(ypred.test2!=test.oj$Purchase)
```
##### The training error for the best model is 0.16, the test error is 0.15.

##### (f)
##### Radial
##### (b)
```{r}
attach(OJ)
svcfit=svm(Purchase~.,data = train.oj, kernel = "radial", cost=0.01,scale = FALSE)
summary(svcfit)
```
##### According to the result, there are 642 supporting vectors in total. Among them, There are two classes, one is CH, the other is MM. There are 327 supporting vectors on one side and 315 on the other side.

##### (c)
```{r}
ypred.train3=predict(svcfit,train.oj)
table(predict=ypred.train3,truth=train.oj$Purchase) 
mean(ypred.train3!=OJ$Purchase[train])
ypred.test3=predict(svcfit,test.oj)
table(predict=ypred.test3,truth=test.oj$Purchase)
mean(ypred.test3!=test.oj$Purchase)
```
##### The training error is 0.39, the test error is 0.38.

##### (d)
```{r}
tune.out = tune(svm, Purchase~., data = train.oj, kernel = "radial", ranges=list(cost=c(0.01,0.1,1,10)))
bestmod2=tune.out$best.model
summary(bestmod2)
```
##### (e)
```{r}
ypred.train4=predict(bestmod2,train.oj)
mean(ypred.train4!=OJ$Purchase[train])
ypred.test4=predict(bestmod2,test.oj)
mean(ypred.test4!=test.oj$Purchase)
```
##### The training error for the best model is 0.15, the test error is 0.19.

##### (f)
##### polynomial
##### (b)
```{r}
svcfit=svm(Purchase~.,data = train.oj, kernel = "polynomial",degree = 2, cost=0.01,scale = FALSE)
summary(svcfit)
```
##### According to the result, there are 333 supporting vectors which have been divided into two classes: CH and MM. There are 166 supporting vectors in one class, and 167 in the other class.

##### (c)
```{r}
ypred.train5=predict(svcfit,train.oj)
table(predict=ypred.train5,truth=train.oj$Purchase)
mean(ypred.train5!=OJ$Purchase[train])
ypred.test5=predict(svcfit,test.oj)
table(predict=ypred.test5,truth=test.oj$Purchase)
mean(ypred.test5!=test.oj$Purchase)
```
##### The training error is 0.165, the test error is 0.159.

##### (d)
```{r}
tune.out = tune(svm, Purchase~., data = train.oj, kernel = "polynomial", ranges=list(cost=c(0.01,0.1,1,10), degree=2))
bestmod3=tune.out$best.model
summary(bestmod3)
```
##### (e)
```{r}
ypred.train6=predict(bestmod3,train.oj)
mean(ypred.train6!=OJ$Purchase[train])
ypred.test6=predict(bestmod3,test.oj)
mean(ypred.test6!=test.oj$Purchase)
```
##### The training error for the best model is 0.15, the test error is 0.189.

##### (h)
##### The lowest test error rate is 0.15, which is generated by support vector classifier using cost=0.1. In conclusion, support vector classifier gives the best results on this data.
